{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "goemotion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQGPe7C2wvd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5abfc1bd-9bcb-4008-8b09-dc1941b08cdf"
      },
      "source": [
        "# Loading drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu5xpaW0w9YK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9c5c127-af5c-40e2-c883-2f6a2719e7ec"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujUJovqw-fJ"
      },
      "source": [
        "# All general imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import SpatialDropout1D\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import io, os, gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUBluvZixGuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ceb97c4a-251c-49b8-9fb2-1c3ff0abbc4b"
      },
      "source": [
        "# Setting the working directory \n",
        "!ls\n",
        "%cd drive/My\\ Drive/Fake_News_Data\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/Fake_News_Data\n",
            "/content/drive/My Drive/Fake_News_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPFnEdMbxJQa"
      },
      "source": [
        "go_train = \"gotrain_bin.tsv\"\n",
        "go_test = \"gotest_bin.tsv\"\n",
        "df_train = pd.read_csv(go_train, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(go_test, sep='\\t', header=None)\n",
        "df_train.columns = ['Text', 'Label', 'ID']\n",
        "df_test.columns = ['Text', 'Label', 'ID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWjFFtNVxV85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "b79c108a-f87e-4fcf-e4d6-ad3465118fdf"
      },
      "source": [
        "print(df_train.head())\n",
        "print(df_test.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Text  Label       ID\n",
            "0                     WHY THE FUCK IS BAYLESS ISOING      1  eezlygj\n",
            "1                        To make her feel threatened      1  ed7ypvh\n",
            "2  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...      1  edvnz26\n",
            "3                                    Fucking coward.      1  edk0z9k\n",
            "4              Stupidly stubborn / stubbornly stupid      1  edkh6qo\n",
            "                                                Text  Label       ID\n",
            "0  Im really sorry about your situation :( Althou...      0  eecwqtt\n",
            "1   Girlfriend weak as well, that jump was pathetic.      0  eeni74k\n",
            "2  I've also heard that intriguing but also kinda...      1  edk4e66\n",
            "3  The thought of shooting anything at asylum see...      1  ed2e00i\n",
            "4  if the pain doesn't go away after 4 hours or s...      0  eezp1cd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqVDABHaxejw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e64ddb6f-1c22-4856-9a72-ac25b757f6b4"
      },
      "source": [
        "train_lst_1 = df_train['Text'].tolist()\n",
        "print(len(train_lst_1))\n",
        "train_lst_1[:5]\n",
        "uq_tr_1 = list(set(train_lst_1))\n",
        "print(len(uq_tr_1))\n",
        "test_lst_1 = df_test['Text'].tolist()\n",
        "uq_ts_1 = list(set(test_lst_1))\n",
        "total_dataset = uq_tr_1 + uq_ts_1\n",
        "print('Dataset length is', len(total_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3731\n",
            "3722\n",
            "Dataset length is 4393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NsdS7N4yCE3"
      },
      "source": [
        "# Defining the tokenizer\n",
        "def get_tokenizer(vocabulary_size):\n",
        "  print('Training tokenizer...')\n",
        "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "  tweet_text = []\n",
        "  print('Read {} Sentences'.format(len(total_dataset)))\n",
        "  tokenizer.fit_on_texts(total_dataset)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0r5kegRyD-R"
      },
      "source": [
        "# For getting the embedding matrix\n",
        "def get_embeddings():\n",
        "  print('Generating embeddings matrix...')\n",
        "  embeddings_file = 'glove.6B.300d.txt'\n",
        "  embeddings_index = dict()\n",
        "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      embeddings_index[word] = vector\n",
        "\t# create a weight matrix for words in training docs\n",
        "  vocabulary_size = len(embeddings_index)\n",
        "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
        "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
        "  tokenizer = get_tokenizer(vocabulary_size)\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
        "  considered = 0\n",
        "  total = len(tokenizer.word_index.items())\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "      print(word, index)\n",
        "      continue\n",
        "    else:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        considered += 1\n",
        "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
        "  return embedding_matrix, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQ_EEIgyGIK"
      },
      "source": [
        "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
        "  print('Loading data')\n",
        "  X1, X2, Y = [], [], []\n",
        "\t# with open(input_file) as infile:\n",
        "\t# \tfor line in infile:\n",
        "\t# \t\tdata = line.split(',')\n",
        "\t# \t\ttext, annotation = data[2], data[1]\n",
        "\t\t\t\n",
        "\t# \t\tif annotation == \"MET\":\n",
        "\t# \t\t\tX.append(text)\n",
        "\t# \t\t\tY.append(\"1\")\n",
        "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
        "\t# \t\t\tX.append(text)\n",
        "\t# \t\t\tY.append(\"0\")\n",
        "  X1 = input_df['Text'].tolist()\n",
        "  Y = input_df['Label'].tolist()\n",
        "  \n",
        "  assert len(X1) == len(Y)\n",
        "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
        "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
        "  Y = np.array(Y)\n",
        "  return X1, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbMoXlccyYWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d9b61897-7a8f-4859-d4dc-1436a603b596"
      },
      "source": [
        "embedding_matrix, tokenizer = get_embeddings()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating embeddings matrix...\n",
            "Vocabulary = 400000, embeddings = 300\n",
            "Training tokenizer...\n",
            "Read 4393 Sentences\n",
            "Considered  6733 Left  525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6DSZy8oyaLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6349f53b-af9d-4659-c5e2-df2908554101"
      },
      "source": [
        "MAX_LENGTH = 40\n",
        "# read ml data\n",
        "X1, Y = get_data(tokenizer, MAX_LENGTH, df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYhYmEo1yeuC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2baad1d9-1058-46a9-d1a1-e968800ed82a"
      },
      "source": [
        "X1_test, Y_test = get_data(tokenizer, MAX_LENGTH, df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCraxuvCyu8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "faf8b670-6668-498b-ff0b-42417b42aaff"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 224, 1: 447})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlrZDXOc09FA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52f46187-064d-4c95-d9cf-16bcf487deb3"
      },
      "source": [
        "Counter(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 1887, 1: 1844})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiCCvgcAykAa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "164cffb3-9003-4f57-a94f-01b036bc1311"
      },
      "source": [
        "y_train = keras.utils.to_categorical(Y)\n",
        "print(y_train)\n",
        "y_test = keras.utils.to_categorical(Y_test)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPXYUomAzy7y"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "VALIDATION_RATIO = 0.1\n",
        "RANDOM_STATE = 9527\n",
        "x1_train, x1_val, \\\n",
        "y_train, y_val = \\\n",
        "    train_test_split(\n",
        "        X1, y_train, \n",
        "        test_size=VALIDATION_RATIO, \n",
        "        random_state=RANDOM_STATE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkbBcsHoz6_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3cb60293-d5a5-4f7d-ad1d-b0fe3ba369e8"
      },
      "source": [
        "print(\"Training Set\")\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_train: {x1_train.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_val:   {x1_val.shape}\")\n",
        "print(f\"y_val :   {y_val.shape}\")\n",
        "print(\"-\" * 10)\n",
        "print(\"Test Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set\n",
            "----------\n",
            "x1_train: (3357, 40)\n",
            "y_train : (3357, 2)\n",
            "----------\n",
            "x1_val:   (374, 40)\n",
            "y_val :   (374, 2)\n",
            "----------\n",
            "Test Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQiX7gvV0BOx"
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "NUM_LSTM_UNITS = 300\n",
        "\n",
        "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
        "\n",
        "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpOim-iG0C2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "9b77fbe5-1188-47f9-bcd5-0edf9dd78754"
      },
      "source": [
        "top_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
        "top_embedded = embedding_layer(\n",
        "    top_input)\n",
        "\n",
        "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
        "top_output = shared_lstm(top_embedded)\n",
        "\n",
        "# merged = concatenate(\n",
        "#     [top_output, bm_output], \n",
        "#     axis=-1)\n",
        "\n",
        "dense_in =  Dense(\n",
        "    units=64, \n",
        "    activation='tanh')\n",
        "\n",
        "intermediate = dense_in(top_output)\n",
        "\n",
        "dense =  Dense(\n",
        "    units=NUM_CLASSES, \n",
        "    activation='softmax')\n",
        "\n",
        "predictions = dense(intermediate)\n",
        "\n",
        "model = Model(\n",
        "    inputs=[top_input], \n",
        "    outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 40)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 40, 300)           120000000 \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 120,740,594\n",
            "Trainable params: 120,740,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPMPpK4m0HBS"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "lr = 1e-3\n",
        "opt = Adam(lr=lr, decay=lr/50)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='goemotion.h5', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMZzTMLA0IXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "b8a0c252-8e77-4ac9-f369-48e09c169750"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
        "history = model.fit(x=x1_train,\n",
        "                    y=y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(\n",
        "                      x1_val, \n",
        "                      y_val\n",
        "                    ),\n",
        "                    shuffle=True,\n",
        "                    callbacks=[checkpointer, stop],\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "27/27 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.6273\n",
            "Epoch 00001: val_loss improved from inf to 0.71337, saving model to goemotion.h5\n",
            "27/27 [==============================] - 38s 1s/step - loss: 0.6390 - accuracy: 0.6273 - val_loss: 0.7134 - val_accuracy: 0.6791\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8826\n",
            "Epoch 00002: val_loss improved from 0.71337 to 0.49420, saving model to goemotion.h5\n",
            "27/27 [==============================] - 40s 1s/step - loss: 0.2975 - accuracy: 0.8826 - val_loss: 0.4942 - val_accuracy: 0.7647\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9628\n",
            "Epoch 00003: val_loss did not improve from 0.49420\n",
            "27/27 [==============================] - 31s 1s/step - loss: 0.1127 - accuracy: 0.9628 - val_loss: 0.5703 - val_accuracy: 0.7861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cLfPbpS0KCq"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model.predict(\n",
        "    [X1_test])\n",
        "# print('Accuracy is')\n",
        "# print(metrics.accuracy_score(y_test, y_pred, sample_weight = reduced_test_weights)*100)\n",
        "# print(classification_report(y_test, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9r4qU7f0Llx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "214e2944-f9c8-4484-bff9-2b75e21909de"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
        "#print(y_pred)\n",
        "print('Accuracy is')\n",
        "print(metrics.accuracy_score(Y_test, y_pred)*100)\n",
        "print(classification_report(Y_test, y_pred, target_names = ['true', 'false']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is\n",
            "78.9865871833085\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        true       0.63      0.89      0.74       224\n",
            "       false       0.93      0.74      0.82       447\n",
            "\n",
            "    accuracy                           0.79       671\n",
            "   macro avg       0.78      0.82      0.78       671\n",
            "weighted avg       0.83      0.79      0.80       671\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rjguiY922gm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "567f24fa-3970-44bc-826f-7fe01bf9479d"
      },
      "source": [
        "type(y_pred)\n",
        "print(len(y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "671\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}