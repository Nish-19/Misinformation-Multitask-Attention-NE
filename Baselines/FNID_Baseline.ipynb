{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNID_Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4jKIhm1AOfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c678c7-de69-48a9-fa49-89fddd7516fb"
      },
      "source": [
        "# Loading drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeH71tyrBSs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bed4636-c4db-427c-e168-d2a7e6c2f278"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH5U5sjaBVAV"
      },
      "source": [
        "# All general imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation, Add\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import SpatialDropout1D\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import io, os, gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl9tyPg9BXfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c5d47d-e605-46fb-8f44-3fc99bd7fbd5"
      },
      "source": [
        "# Setting the working directory \n",
        "!ls\n",
        "%cd drive/My\\ Drive/Fake_News_Data\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/Fake_News_Data\n",
            "/content/drive/My Drive/Fake_News_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNfqa33JBZ6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "outputId": "a2511c6b-24b1-4b0c-8f69-dc3bacb5d0a5"
      },
      "source": [
        "#################### Importing FNID Datasets ####################\n",
        "# Train set\n",
        "train_df = pd.read_csv('fnn_train.csv')\n",
        "print(train_df.columns)\n",
        "train_df.head()\n",
        "le = LabelEncoder()\n",
        "train_df['label_fnn'] = le.fit_transform(train_df['label_fnn'])\n",
        "train_df.head()\n",
        "\n",
        "# dev set\n",
        "dev_df = pd.read_csv('fnn_dev.csv')\n",
        "print(dev_df.columns)\n",
        "dev_df['label_fnn'] = le.transform(dev_df['label_fnn'])\n",
        "dev_df.head()\n",
        "\n",
        "# Test set\n",
        "test_df = pd.read_csv('fnn_test.csv')\n",
        "print(test_df.columns)\n",
        "test_df['label_fnn'] = le.transform(test_df['label_fnn'])\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
            "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
            "       'best_novelty', 'best_emotion'],\n",
            "      dtype='object')\n",
            "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
            "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
            "       'best_novelty', 'best_emotion'],\n",
            "      dtype='object')\n",
            "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
            "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
            "       'best_novelty', 'best_emotion'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>speaker</th>\n",
              "      <th>statement</th>\n",
              "      <th>sources</th>\n",
              "      <th>paragraph_based_content</th>\n",
              "      <th>fullText_based_content</th>\n",
              "      <th>label_fnn</th>\n",
              "      <th>best_novelty</th>\n",
              "      <th>best_emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1678</td>\n",
              "      <td>2010-04-11T16:37:40-04:00</td>\n",
              "      <td>Jon Kyl</td>\n",
              "      <td>\"President Obama himself attempted to filibust...</td>\n",
              "      <td>['http://abcnews.go.com/ThisWeek/video/supreme...</td>\n",
              "      <td>['U.S. Supreme Court Justice John Paul Stevens...</td>\n",
              "      <td>U.S. Supreme Court Justice John Paul Stevens a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1820</td>\n",
              "      <td>2010-05-23T18:11:09-04:00</td>\n",
              "      <td>Michael Steele</td>\n",
              "      <td>In Hawaii, \"they don't have a history of throw...</td>\n",
              "      <td>['http://www.starbulletin.com/news/bulletin/94...</td>\n",
              "      <td>[\"On ABC's This Week, the chairmen of the Repu...</td>\n",
              "      <td>On ABC's This Week, the chairmen of the Republ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1624</td>\n",
              "      <td>2010-03-26T10:24:21-04:00</td>\n",
              "      <td>John Boehner</td>\n",
              "      <td>\"Our national debt ... is on track to exceed t...</td>\n",
              "      <td>['http://www.desmoinesregister.com/article/201...</td>\n",
              "      <td>['Ever since Barack Obama became president and...</td>\n",
              "      <td>Ever since Barack Obama became president and b...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1576</td>\n",
              "      <td>2010-03-12T11:45:14-05:00</td>\n",
              "      <td>America's Health Insurance Plans</td>\n",
              "      <td>\"Health insurance companies' costs are only 4 ...</td>\n",
              "      <td>['http://www.youtube.com/watch?v=4O8CxZ1OD58',...</td>\n",
              "      <td>[\"As the battle over health care reform approa...</td>\n",
              "      <td>As the battle over health care reform approach...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1770</td>\n",
              "      <td>2010-05-07T11:54:44-04:00</td>\n",
              "      <td>Michael Bloomberg</td>\n",
              "      <td>\"We can prevent terror suspects from boarding ...</td>\n",
              "      <td>['http://www.huffingtonpost.com/michael-bloomb...</td>\n",
              "      <td>['In the wake of a foiled car bomb attempt in ...</td>\n",
              "      <td>In the wake of a foiled car bomb attempt in Ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                       date  ... best_novelty best_emotion\n",
              "0  1678  2010-04-11T16:37:40-04:00  ...            0            0\n",
              "1  1820  2010-05-23T18:11:09-04:00  ...            1            1\n",
              "2  1624  2010-03-26T10:24:21-04:00  ...            1            0\n",
              "3  1576  2010-03-12T11:45:14-05:00  ...            1            0\n",
              "4  1770  2010-05-07T11:54:44-04:00  ...            1            1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOBTbc8gB0um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7096355b-a924-44a3-b166-f6f7a3b67150"
      },
      "source": [
        "train_lst_1 = train_df['statement'].tolist()\n",
        "train_lst_2 = train_df['paragraph_based_content']\n",
        "uq_tr_1 = list(set(train_lst_1))\n",
        "uq_tr_2 = list(set(train_lst_2))\n",
        "print(len(uq_tr_1))\n",
        "train_merged = uq_tr_1 + uq_tr_2\n",
        "print('Train Length is', len(train_merged))\n",
        "test_lst_1 = test_df['statement'].tolist()\n",
        "test_lst_2 = test_df['paragraph_based_content']\n",
        "uq_ts_1 = list(set(test_lst_1))\n",
        "uq_ts_2 = list(set(test_lst_2))\n",
        "test_merged = uq_ts_1 + uq_ts_2\n",
        "dev_lst_1 = dev_df['statement'].tolist()\n",
        "dev_lst_2 = dev_df['paragraph_based_content']\n",
        "uq_dv_1 = list(set(dev_lst_1))\n",
        "uq_dv_2 = list(set(dev_lst_2))\n",
        "dev_merged = uq_dv_1 + uq_dv_2\n",
        "total_dataset = train_merged + dev_merged + test_merged\n",
        "print('Dataset length is', len(total_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15204\n",
            "Train Length is 30415\n",
            "Dataset length is 34638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j57scYihBoKc"
      },
      "source": [
        "# Defining the tokenizer\n",
        "def get_tokenizer(vocabulary_size):\n",
        "  print('Training tokenizer...')\n",
        "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "  tweet_text = []\n",
        "  print('Read {} Sentences'.format(len(total_dataset)))\n",
        "  tokenizer.fit_on_texts(total_dataset)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJHuYleQB-F2"
      },
      "source": [
        "# For getting the embedding matrix\n",
        "def get_embeddings():\n",
        "  print('Generating embeddings matrix...')\n",
        "  embeddings_file = 'glove.6B.300d.txt'\n",
        "  embeddings_index = dict()\n",
        "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      embeddings_index[word] = vector\n",
        "\t# create a weight matrix for words in training docs\n",
        "  vocabulary_size = len(embeddings_index)\n",
        "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
        "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
        "  tokenizer = get_tokenizer(vocabulary_size)\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
        "  considered = 0\n",
        "  total = len(tokenizer.word_index.items())\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "      print(word, index)\n",
        "      continue\n",
        "    else:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        considered += 1\n",
        "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
        "  return embedding_matrix, tokenizer, embeddings_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HPijJ8iCBvV"
      },
      "source": [
        "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
        "  print('Loading data')\n",
        "  X1, X2, Y = [], [], []\n",
        "\t# with open(input_file) as infile:\n",
        "\t# \tfor line in infile:\n",
        "\t# \t\tdata = line.split(',')\n",
        "\t# \t\ttext, annotation = data[2], data[1]\n",
        "\t\t\t\n",
        "\t# \t\tif annotation == \"MET\":\n",
        "\t# \t\t\tX.append(text)\n",
        "\t# \t\t\tY.append(\"1\")\n",
        "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
        "\t# \t\t\tX.append(text)\n",
        "\t# \t\t\tY.append(\"0\")\n",
        "  X1 = input_df['paragraph_based_content'].tolist()\n",
        "  X2 = input_df['statement'].tolist()\n",
        "  Y = input_df['label_fnn'].tolist()\n",
        "  \n",
        "  assert len(X1) == len(X2) == len(Y)\n",
        "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
        "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
        "\t# for i, s in enumerate(sequences):\n",
        "\t# \tsequences[i] = sequences[i][-250:]\n",
        "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
        "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
        "  Y_fnd = np.array(Y)\n",
        "\n",
        "  return X1, X2, Y_fnd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSELmRRfCfH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e103758-c985-4cea-921c-68626ae0f975"
      },
      "source": [
        "embedding_matrix, tokenizer, embeddings_index = get_embeddings()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating embeddings matrix...\n",
            "Vocabulary = 400000, embeddings = 300\n",
            "Training tokenizer...\n",
            "Read 34638 Sentences\n",
            "Considered  62687 Left  49697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEyrwXjMCjKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6138218d-5ad6-49c4-da42-21ec70c858f5"
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "# read ml data\n",
        "X1, X2, Y_fnd = get_data(tokenizer, MAX_LENGTH, train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBJdMKRh1IOA",
        "outputId": "c3fb805e-4eb6-43ce-bf5a-f4dce4c87d5d"
      },
      "source": [
        "X1_val, X2_val, Y_fnd_val = get_data(tokenizer, MAX_LENGTH, dev_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qtttbb0Cn-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7252fe6c-c196-457b-d9ae-c93a8cabd3f7"
      },
      "source": [
        "X1_test, X2_test, Y_fnd_test = get_data(tokenizer, MAX_LENGTH, test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcaDeb9tE083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e877290e-a840-4850-e688-8cdc30509dde"
      },
      "source": [
        "# Creating one-hot encodings\n",
        "y_train_fnd = keras.utils.to_categorical(Y_fnd)\n",
        "print(y_train_fnd)\n",
        "y_test_fnd = keras.utils.to_categorical(Y_fnd_test)\n",
        "print(y_test_fnd)\n",
        "y_val_fnd = keras.utils.to_categorical(Y_fnd_val)\n",
        "print(y_val_fnd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxug_2MFMVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431adb39-3832-4865-cc60-8ee1ae495405"
      },
      "source": [
        "print(\"Training Set\")\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_train: {X1.shape}\")\n",
        "print(f\"x2_train: {X2.shape}\")\n",
        "print(f\"y_train_cs : {y_train_fnd.shape}\")\n",
        "\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_val:   {X1_val.shape}\")\n",
        "print(f\"x2_val:   {X2_val.shape}\")\n",
        "print(f\"y_val_cs :   {y_val_fnd.shape}\")\n",
        "print(\"-\" * 10)\n",
        "print(\"Test Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set\n",
            "----------\n",
            "x1_train: (15212, 100)\n",
            "x2_train: (15212, 100)\n",
            "y_train_cs : (15212, 2)\n",
            "----------\n",
            "x1_val:   (1058, 100)\n",
            "x2_val:   (1058, 100)\n",
            "y_val_cs :   (1058, 2)\n",
            "----------\n",
            "Test Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naPNMlWxFRjW"
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "NUM_LSTM_UNITS = 128\n",
        "\n",
        "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
        "\n",
        "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF-xT_ThX14R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbf8249-e769-4b2d-97cb-576f6ab950cc"
      },
      "source": [
        "print('Getting Text CNN model...')\n",
        "filter_sizes = [2, 3, 5]\n",
        "num_filters = 128\t#Hyperparameters 32,64,128; 0.2,0.3,0.4\n",
        "drop = 0.4\n",
        "top_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "bm_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
        "top_embedded = embedding_layer(\n",
        "    top_input)\n",
        "bm_embedded = embedding_layer(\n",
        "    bm_input)\n",
        "reshape = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(top_embedded)\n",
        "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(bm_embedded)\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape_1)\n",
        "#conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim),  padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "#maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "predictions = Dense(units=NUM_CLASSES, activation='sigmoid')(dropout)\n",
        "\n",
        "model = Model(\n",
        "    inputs=[top_input, bm_input], \n",
        "    outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Text CNN model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvR4vF2XGUAA"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "lr = 1e-3\n",
        "opt = Adam(lr=lr, decay=lr/50)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IehPCX70YbdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23527c90-8f33-4db5-8170-63027eae43c0"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 50\n",
        "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
        "history = model.fit(x=[X1, X2],\n",
        "                    y=y_train_fnd,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(\n",
        "                      [X1_val, X2_val], \n",
        "                      y_val_fnd\n",
        "                    ),\n",
        "                    shuffle=True,\n",
        "                    callbacks=stop,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 43s 1s/step - loss: 0.6907 - accuracy: 0.5312 - val_loss: 0.6398 - val_accuracy: 0.6796\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.5968 - accuracy: 0.7130 - val_loss: 0.5946 - val_accuracy: 0.6909\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.4931 - accuracy: 0.7946 - val_loss: 0.5758 - val_accuracy: 0.6957\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.3820 - accuracy: 0.8644 - val_loss: 0.5820 - val_accuracy: 0.6871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4XksIFtGugN"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model.predict(\n",
        "    [X1_test, X2_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEA73dfiGxVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e58e73d-bd18-48a1-9945-84634256829b"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
        "print('FNID Accuracy is')\n",
        "print(metrics.accuracy_score(Y_fnd_test, y_pred)*100)\n",
        "print(classification_report(Y_fnd_test, y_pred, target_names = ['fake', 'real']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FNID Accuracy is\n",
            "79.79127134724858\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.67      0.96      0.79       418\n",
            "        real       0.96      0.69      0.81       636\n",
            "\n",
            "    accuracy                           0.80      1054\n",
            "   macro avg       0.82      0.83      0.80      1054\n",
            "weighted avg       0.85      0.80      0.80      1054\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6oln1U_NbMx",
        "outputId": "24dd3147-022c-4089-a04d-4d909ab44b35"
      },
      "source": [
        "# from keras import Input\r\n",
        "# from keras.layers import Embedding,LSTM, concatenate, Dense\r\n",
        "# from keras.models import Model\r\n",
        "\r\n",
        "top_input = Input(\r\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \r\n",
        "    dtype='int32')\r\n",
        "bm_input = Input(\r\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \r\n",
        "    dtype='int32')\r\n",
        "\r\n",
        "embedding_layer = Embedding(\r\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\r\n",
        "top_embedded = embedding_layer(\r\n",
        "    top_input)\r\n",
        "bm_embedded = embedding_layer(\r\n",
        "    bm_input)\r\n",
        "\r\n",
        "shared_lstm = LSTM(NUM_LSTM_UNITS)\r\n",
        "top_output = shared_lstm(top_embedded)\r\n",
        "bm_output = shared_lstm(bm_embedded)\r\n",
        "\r\n",
        "merged = concatenate(\r\n",
        "    [top_output, bm_output], \r\n",
        "    axis=-1)\r\n",
        "\r\n",
        "dense =  Dense(\r\n",
        "    units=NUM_CLASSES, \r\n",
        "    activation='softmax')\r\n",
        "predictions = dense(merged)\r\n",
        "\r\n",
        "model = Model(\r\n",
        "    inputs=[top_input, bm_input], \r\n",
        "    outputs=predictions)\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 100, 300)     120000000   input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          219648      embedding_2[0][0]                \n",
            "                                                                 embedding_2[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256)          0           lstm[0][0]                       \n",
            "                                                                 lstm[1][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            514         concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 120,220,162\n",
            "Trainable params: 120,220,162\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU0JFbb2Nd0m"
      },
      "source": [
        "from keras.optimizers import Adam\r\n",
        "lr = 1e-3\r\n",
        "opt = Adam(lr=lr, decay=lr/50)\r\n",
        "model.compile(\r\n",
        "    optimizer='adam',\r\n",
        "    loss='categorical_crossentropy',\r\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZcD4B1XNe3y",
        "outputId": "52c56d40-bc38-47f4-aa53-33668fe1cd68"
      },
      "source": [
        "BATCH_SIZE = 512\r\n",
        "NUM_EPOCHS = 50\r\n",
        "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\r\n",
        "history = model.fit(x=[X1, X2],\r\n",
        "                    y=y_train_fnd,\r\n",
        "                    batch_size=BATCH_SIZE,\r\n",
        "                    epochs=NUM_EPOCHS,\r\n",
        "                    validation_data=(\r\n",
        "                      [X1_val, X2_val], \r\n",
        "                      y_val_fnd\r\n",
        "                    ),\r\n",
        "                    shuffle=True,\r\n",
        "                    callbacks=stop,\r\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 38s 1s/step - loss: 0.6803 - accuracy: 0.5624 - val_loss: 0.6150 - val_accuracy: 0.6664\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 34s 1s/step - loss: 0.5273 - accuracy: 0.7472 - val_loss: 0.6017 - val_accuracy: 0.6711\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 34s 1s/step - loss: 0.3179 - accuracy: 0.8726 - val_loss: 0.7802 - val_accuracy: 0.6597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQhMk-F1NlZu"
      },
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "predictions = model.predict(\r\n",
        "    [X1_test, X2_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3shaTXtNodP",
        "outputId": "92ef4e90-5bf8-4075-cc41-ed1cac40f1cd"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\r\n",
        "print('FNID Accuracy is')\r\n",
        "print(metrics.accuracy_score(Y_fnd_test, y_pred)*100)\r\n",
        "print(classification_report(Y_fnd_test, y_pred, target_names = ['fake', 'real']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FNID Accuracy is\n",
            "74.38330170777988\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.62      0.91      0.74       418\n",
            "        real       0.91      0.64      0.75       636\n",
            "\n",
            "    accuracy                           0.74      1054\n",
            "   macro avg       0.77      0.77      0.74      1054\n",
            "weighted avg       0.80      0.74      0.74      1054\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}