{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mv53mLhtfbA6",
    "outputId": "54ba21ab-7498-4c7d-d213-51b37f2a8cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVb9M-KtfeH6"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "gKTq7DodfjWL",
    "outputId": "85b09d00-e1ce-4a31-e29e-6715d19c72dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'label'],\n",
      "      dtype='object')\n",
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'Expected', 'Weight', 'Usage'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>167564</td>\n",
       "      <td>160994</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The hanging Saddam is a surrogate? This man's ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>167564</td>\n",
       "      <td>15084</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>Chinese loquat loquat plaster in America? Pure...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    tid1    tid2  ... Expected  Weight    Usage\n",
       "0  321187  167562   59521  ...        2  0.0625  Private\n",
       "1  321190  167564   91315  ...        2  0.0625   Public\n",
       "2  321189  167563  167564  ...        2  0.0625  Private\n",
       "3  321193  167564  160994  ...        2  0.0625   Public\n",
       "4  321191  167564   15084  ...        2  0.0625   Public\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('../ByteDance_Dataset/train.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['bd_label'] = le.fit_transform(train_df['bd_label'])\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('../ByteDance_Dataset/test_merged.csv')\n",
    "print(test_df.columns)\n",
    "test_df['bd_label'] = le.transform(test_df['bd_label'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "2mt6r5bBmiWA",
    "outputId": "c729c9f1-ac17-463d-d7eb-b3b7971f259e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320552\n",
      "320552\n",
      "67869\n",
      "136111\n",
      "Train Length is 203980\n",
      "Test merged 68725\n",
      "Dataset length is 272705\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['title1_en'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "train_lst_2 = train_df['title2_en'].tolist()\n",
    "print(len(train_lst_2))\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "print(len(uq_tr_2))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['title1_en'].tolist()\n",
    "test_lst_2 = test_df['title2_en'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZdPhwWrhdLG"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OadfRX-4gU8S"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = '../resources/glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9uPkbC-utVw"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df, train=True):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "\t# with open(input_file) as infile:\n",
    "\t# \tfor line in infile:\n",
    "\t# \t\tdata = line.split(',')\n",
    "\t# \t\ttext, annotation = data[2], data[1]\n",
    "\t\t\t\n",
    "\t# \t\tif annotation == \"MET\":\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"1\")\n",
    "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"0\")\n",
    "  X1 = input_df['title1_en'].tolist()\n",
    "  X2 = input_df['title2_en'].tolist()\n",
    "  if train:\n",
    "    Y = input_df['label'].tolist()\n",
    "  else:\n",
    "    Y = input_df['Expected'].tolist()\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "\t# for i, s in enumerate(sequences):\n",
    "\t# \tsequences[i] = sequences[i][-250:]\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y = np.array(Y)\n",
    "  return X1, X2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "VAga5b68yVd1",
    "outputId": "f35e292f-bfd9-419c-cba3-d376eb15c12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 272705 Sentences\n",
      "Considered  32590 Left  14439\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Gy7eZFtzcPS",
    "outputId": "5107db66-86f4-4e55-d97e-2f3d578dc89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "# read ml data\n",
    "X1, X2, Y = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XJGcHZj47qgH",
    "outputId": "fd720619-daff-4772-e7ab-bef0d7d22074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_test = get_data(tokenizer, MAX_LENGTH, test_df, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6ncoMw_g7TQI",
    "outputId": "3740337a-cf9e-4bc7-a08e-25adf6b57f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320552,)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wPvxwrgGztW5",
    "outputId": "92cfb324-fc48-4d9c-cec1-f74aae94bd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(320552, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X1))\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "2PzQhVcl0SQ0",
    "outputId": "03317f80-a560-477d-90e8-fdfe7923ab50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()#convertes into one hot form\n",
    "encoder.fit(Y)\n",
    "Y_enc = encoder.transform(Y)\n",
    "print(Y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "uLo6gOj_5Unq",
    "outputId": "11d2463e-c820-4ca7-d497-cf74f73d84d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (101239, 20)\n",
      "Train labels [0 0 0 ... 1 0 0]\n",
      "Test shape (28746, 20)\n",
      "Test labels [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Have to eliminate unrealted labels\n",
    "# Removing the unrelated samples from both train and test\n",
    "result = np.where(train_df['bd_label'] == 2)\n",
    "reduced_X1 = np.delete(X1, result[0], axis=0)\n",
    "reduced_X2 = np.delete(X2, result[0], axis=0)\n",
    "print('Train shape', reduced_X1.shape)\n",
    "reduced_train_labels = np.delete(train_df['bd_label'].values, result[0])\n",
    "print('Train labels', reduced_train_labels)\n",
    "assert len(reduced_X1) == len(reduced_X2) == len(reduced_train_labels)\n",
    "result_test = np.where(test_df['bd_label']==2)\n",
    "reduced_X1_test = np.delete(X1_test, result_test[0], axis=0)\n",
    "reduced_X2_test = np.delete(X2_test, result_test[0], axis=0)\n",
    "print('Test shape', reduced_X1_test.shape)\n",
    "reduced_test_labels = np.delete(test_df['bd_label'].values, result_test[0])\n",
    "reduced_test_weights = np.delete(test_df['Weight'].values, result_test[0])\n",
    "print('Test labels', reduced_test_labels)\n",
    "assert len(reduced_X1_test) == len(reduced_X2_test) == len(reduced_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "sJp5NKvd6xXA",
    "outputId": "c28f2deb-1c22-4895-c087-3d917221199d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(101239,)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(reduced_train_labels))\n",
    "print(reduced_train_labels.shape)\n",
    "encoder = LabelBinarizer()#convertes into one hot form\n",
    "encoder.fit(reduced_train_labels)\n",
    "Y_enc = encoder.transform(reduced_train_labels)\n",
    "Y_enc_test = encoder.transform(reduced_test_labels)\n",
    "print(Y_enc)\n",
    "print(Y_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "UpnazOfI7g3U",
    "outputId": "7bc82407-b13d-4853-de3b-45fd9e6d3736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(reduced_train_labels)\n",
    "print(y_train)\n",
    "y_test = keras.utils.to_categorical(reduced_test_labels)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwWaDbzc-e-L"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        reduced_X1, reduced_X1, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "nxsekBsB-2J3",
    "outputId": "7d429ee4-f830-4f7a-8553-649f976099c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (91115, 20)\n",
      "x2_train: (91115, 20)\n",
      "y_train : (91115, 2)\n",
      "----------\n",
      "x1_val:   (10124, 20)\n",
      "x2_val:   (10124, 20)\n",
      "y_val :   (10124, 2)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NowVvChI_S9Z"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "jRsGiCux_3YA",
    "outputId": "01381711-a1d0-49ca-dd5b-51c59124aad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 300)      120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          219648      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            514         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 120,220,162\n",
      "Trainable params: 120,220,162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras import Input\n",
    "# from keras.layers import Embedding,LSTM, concatenate, Dense\n",
    "# from keras.models import Model\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Y3pD2sA5YP"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "colab_type": "code",
    "id": "JuxjGECRBbP3",
    "outputId": "230d894c-7ce5-471c-acf2-d02ac447e156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "178/178 [==============================] - 193s 1s/step - loss: 0.1880 - accuracy: 0.9407 - val_loss: 0.1321 - val_accuracy: 0.9559\n",
      "Epoch 2/50\n",
      "178/178 [==============================] - 189s 1s/step - loss: 0.1012 - accuracy: 0.9657 - val_loss: 0.1266 - val_accuracy: 0.9598\n",
      "Epoch 3/50\n",
      "178/178 [==============================] - 187s 1s/step - loss: 0.0759 - accuracy: 0.9736 - val_loss: 0.1214 - val_accuracy: 0.9625\n",
      "Epoch 4/50\n",
      "178/178 [==============================] - 186s 1s/step - loss: 0.0595 - accuracy: 0.9778 - val_loss: 0.1370 - val_accuracy: 0.9636\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6zQysQjEAAt"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [reduced_X1_test, reduced_X2_test])\n",
    "# print('Accuracy is')\n",
    "# print(metrics.accuracy_score(y_test, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "# print(classification_report(y_test, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UtOIr7_QgeD"
   },
   "outputs": [],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "#print(y_pred)\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(reduced_test_labels, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BD_Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
