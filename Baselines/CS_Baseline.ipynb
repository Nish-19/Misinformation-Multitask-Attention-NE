{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS_Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LLeZCWv8yAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9f30ef3-b465-4b52-e2c5-db7fc4966140"
      },
      "source": [
        "# Loading drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oudz57hb9cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c049f8aa-9a54-4a06-df35-e866ab88c5ae"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0FcIrBP9eUj"
      },
      "source": [
        "# All general imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import SpatialDropout1D\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import io, os, gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUfh66UG9gUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3243c171-4361-4da2-e95c-8d1385ea7374"
      },
      "source": [
        "# Setting the working directory \n",
        "!ls\n",
        "%cd drive/My\\ Drive/Fake_News_Data\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/Fake_News_Data\n",
            "/content/drive/My Drive/Fake_News_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF8xIUuS9iHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "f77be02a-56c7-4aa5-dfb6-ffc6e513961c"
      },
      "source": [
        "#################### Importing ByteDance Datasets ####################\n",
        "# Train set\n",
        "train_df = pd.read_csv('cstance_train.csv')\n",
        "print(train_df.columns)\n",
        "train_df.head()\n",
        "\n",
        "# Test set\n",
        "test_df = pd.read_csv('cstance_test_new.csv')\n",
        "print(test_df.columns)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'text', 'stance'], dtype='object')\n",
            "Index(['id', 'text', 'stance'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1248863218644156416</td>\n",
              "      <td>what is the reason the cdc is hiding info re h...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1248863575009083395</td>\n",
              "      <td>so you are denying that dr vladimir zelenko is...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1248863726717095936</td>\n",
              "      <td>anyone whos been to a malaria area would in th...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1255980263676489735</td>\n",
              "      <td>many countries preceded us in adopting chloroq...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1255985637640945664</td>\n",
              "      <td>those who proselytize against hydroxychloroqui...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id  ... stance\n",
              "0  1248863218644156416  ...    2.0\n",
              "1  1248863575009083395  ...    2.0\n",
              "2  1248863726717095936  ...    2.0\n",
              "3  1255980263676489735  ...    2.0\n",
              "4  1255985637640945664  ...    2.0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzdKL3Bc9-lP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c83104f9-71d5-4c8a-f464-c77a403a3ab9"
      },
      "source": [
        "premise = [\"chloroquine hydroxychloroquine are cure for the novel coronavirus\"]\n",
        "train_lst_1 = train_df['text'].tolist()\n",
        "print(len(train_lst_1))\n",
        "train_lst_1[:5]\n",
        "uq_tr_1 = list(set(train_lst_1))\n",
        "print(len(uq_tr_1))\n",
        "train_merged = uq_tr_1 + premise\n",
        "print('Train Length is', len(train_merged))\n",
        "train_merged[:5]\n",
        "test_lst_1 = test_df['text'].tolist()\n",
        "uq_ts_1 = list(set(test_lst_1))\n",
        "test_merged = uq_ts_1\n",
        "print('Test merged', len(test_merged))\n",
        "total_dataset = train_merged + test_merged\n",
        "print('Dataset length is', len(total_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8572\n",
            "7185\n",
            "Train Length is 7186\n",
            "Test merged 2366\n",
            "Dataset length is 9552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2ji_iSK-wCH"
      },
      "source": [
        "# Defining the tokenizer\n",
        "def get_tokenizer(vocabulary_size):\n",
        "  print('Training tokenizer...')\n",
        "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "  tweet_text = []\n",
        "  print('Read {} Sentences'.format(len(total_dataset)))\n",
        "  tokenizer.fit_on_texts(total_dataset)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt3weWiE-6xL"
      },
      "source": [
        "# For getting the embedding matrix\n",
        "def get_embeddings():\n",
        "  print('Generating embeddings matrix...')\n",
        "  embeddings_file = 'glove.6B.300d.txt'\n",
        "  embeddings_index = dict()\n",
        "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      embeddings_index[word] = vector\n",
        "\t# create a weight matrix for words in training docs\n",
        "  vocabulary_size = len(embeddings_index)\n",
        "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
        "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
        "  tokenizer = get_tokenizer(vocabulary_size)\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
        "  considered = 0\n",
        "  total = len(tokenizer.word_index.items())\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "      print(word, index)\n",
        "      continue\n",
        "    else:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        considered += 1\n",
        "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
        "  return embedding_matrix, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtUOTDMQ-9Nb"
      },
      "source": [
        "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
        "  print('Loading data')\n",
        "  X1, X2, Y = [], [], []\n",
        "  X2 = input_df['text'].tolist()\n",
        "  length = len(X2)\n",
        "  premise = \"chloroquine hydroxychloroquine are cure for the novel coronavirus\"\n",
        "  X1 = [premise for i in range(length)]\n",
        "  Y = input_df['stance'].tolist()\n",
        "  new_Y = [(ele-1) for ele in Y]\n",
        "  assert len(new_Y) == len(Y)\n",
        "  \n",
        "  len(X1) == len(X2) == len(Y)\n",
        "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
        "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
        "\t# for i, s in enumerate(sequences):\n",
        "\t# \tsequences[i] = sequences[i][-250:]\n",
        "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
        "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
        "  new_Y = np.array(new_Y)\n",
        "  return X1, X2, new_Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U04e1ovhAC_2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "580fff95-a98a-40af-ed0f-d633bc752921"
      },
      "source": [
        "embedding_matrix, tokenizer = get_embeddings()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating embeddings matrix...\n",
            "Vocabulary = 400000, embeddings = 300\n",
            "Training tokenizer...\n",
            "Read 9552 Sentences\n",
            "Considered  11908 Left  4553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5EQtJ0RAGPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5954b91c-64ab-4b59-a107-67dba5a486ce"
      },
      "source": [
        "MAX_LENGTH = 50\n",
        "# read ml data\n",
        "X1, X2, Y = get_data(tokenizer, MAX_LENGTH, train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJx3GbwpAX01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ef14851-2e47-4feb-c072-bb982ca86305"
      },
      "source": [
        "X1_test, X2_test, Y_test = get_data(tokenizer, MAX_LENGTH, test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFY-6k5fAfW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7488e9f-56e5-424a-f9bb-8ca414a97e64"
      },
      "source": [
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8572,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqUmSuKlBAA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dc598ab9-3f2d-4d16-aa90-3d98ab52301b"
      },
      "source": [
        "print(type(X1))\n",
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8572, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktnd2djgBCZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "412b315e-495f-4e49-9f94-826d55cfae47"
      },
      "source": [
        "encoder = LabelBinarizer()#convertes into one hot form\n",
        "encoder.fit(Y)\n",
        "Y_enc = encoder.transform(Y)\n",
        "Y_enc_test = encoder.transform(Y_test)\n",
        "print(Y_enc)\n",
        "print(Y_enc_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIEgsJLCBuWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "87c360aa-f5c3-47e5-fd86-9da605fe7055"
      },
      "source": [
        "y_train = keras.utils.to_categorical(Y)\n",
        "print(y_train)\n",
        "y_test = keras.utils.to_categorical(Y_test)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITJnAsF3B8x6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "VALIDATION_RATIO = 0.1\n",
        "RANDOM_STATE = 9527\n",
        "x1_train, x1_val, \\\n",
        "x2_train, x2_val, \\\n",
        "y_train, y_val = \\\n",
        "    train_test_split(\n",
        "        X1, X2, y_train, \n",
        "        test_size=VALIDATION_RATIO, \n",
        "        random_state=RANDOM_STATE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJu-0RcNBFQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "0c8c5241-22d9-4f9d-c591-a15b5217c13a"
      },
      "source": [
        "print(\"Training Set\")\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_train: {x1_train.shape}\")\n",
        "print(f\"x2_train: {x2_train.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_val:   {x1_val.shape}\")\n",
        "print(f\"x2_val:   {x2_val.shape}\")\n",
        "print(f\"y_val :   {y_val.shape}\")\n",
        "print(\"-\" * 10)\n",
        "print(\"Test Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set\n",
            "----------\n",
            "x1_train: (7714, 50)\n",
            "x2_train: (7714, 50)\n",
            "y_train : (7714, 2)\n",
            "----------\n",
            "x1_val:   (858, 50)\n",
            "x2_val:   (858, 50)\n",
            "y_val :   (858, 2)\n",
            "----------\n",
            "Test Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhjctoX7CpbT"
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "\n",
        "NUM_LSTM_UNITS = 128\n",
        "\n",
        "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
        "\n",
        "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEd_DNLYPZy3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10b5800d-d6e3-449e-c402-6b8e9fa2e499"
      },
      "source": [
        "print('Getting Text CNN model...')\n",
        "filter_sizes = [2, 3, 5]\n",
        "num_filters = 128\t#Hyperparameters 32,64,128; 0.2,0.3,0.4\n",
        "drop = 0.4\n",
        "top_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "bm_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
        "top_embedded = embedding_layer(\n",
        "    top_input)\n",
        "bm_embedded = embedding_layer(\n",
        "    bm_input)\n",
        "reshape = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(top_embedded)\n",
        "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(bm_embedded)\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape_1)\n",
        "#conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim),  padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "#maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "predictions = Dense(units=NUM_CLASSES, activation='sigmoid')(dropout)\n",
        "\n",
        "model = Model(\n",
        "    inputs=[top_input, bm_input], \n",
        "    outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Text CNN model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeRCG6iXPdSU"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "lr = 1e-3\n",
        "opt = Adam(lr=lr, decay=lr/50)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQn6CvT8Pdw0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "aaf51704-ae88-4b60-b82a-efef39617c30"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 50\n",
        "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
        "history = model.fit(x=[x1_train, x2_train],\n",
        "                    y=y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(\n",
        "                      [x1_val, x2_val], \n",
        "                      y_val\n",
        "                    ),\n",
        "                    shuffle=True,\n",
        "                    callbacks=stop,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.6606 - accuracy: 0.5852 - val_loss: 0.6157 - val_accuracy: 0.6364\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.5634 - accuracy: 0.7200 - val_loss: 0.4890 - val_accuracy: 0.8112\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.3983 - accuracy: 0.8636 - val_loss: 0.3668 - val_accuracy: 0.8415\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.2626 - accuracy: 0.9121 - val_loss: 0.3065 - val_accuracy: 0.8671\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.1745 - accuracy: 0.9468 - val_loss: 0.2862 - val_accuracy: 0.8671\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.1180 - accuracy: 0.9669 - val_loss: 0.2914 - val_accuracy: 0.8706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKKSzqzpPtF4"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model.predict(\n",
        "    [X1_test, X2_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-nnkpP-P0W1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "4e338514-46a9-4cf2-e326-2c758034d041"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
        "test_labels = test_df['stance'].tolist()\n",
        "n_test_labels = [(ele-1) for ele in test_labels]\n",
        "#print(y_pred)\n",
        "print('Accuracy is')\n",
        "print(metrics.accuracy_score(n_test_labels, y_pred)*100)\n",
        "print(classification_report(n_test_labels, y_pred, target_names = ['against', 'for']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is\n",
            "85.16439454691259\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     against       0.84      0.78      0.81      1007\n",
            "         for       0.86      0.90      0.88      1487\n",
            "\n",
            "    accuracy                           0.85      2494\n",
            "   macro avg       0.85      0.84      0.84      2494\n",
            "weighted avg       0.85      0.85      0.85      2494\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-I80h-ENx6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "66e7635f-63bc-46e6-fb43-f8b35b12376b"
      },
      "source": [
        "# from keras import Input\n",
        "# from keras.layers import Embedding,LSTM, concatenate, Dense\n",
        "# from keras.models import Model\n",
        "\n",
        "top_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "bm_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
        "top_embedded = embedding_layer(\n",
        "    top_input)\n",
        "bm_embedded = embedding_layer(\n",
        "    bm_input)\n",
        "\n",
        "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
        "top_output = shared_lstm(top_embedded)\n",
        "bm_output = shared_lstm(bm_embedded)\n",
        "\n",
        "merged = concatenate(\n",
        "    [top_output, bm_output], \n",
        "    axis=-1)\n",
        "\n",
        "dense =  Dense(\n",
        "    units=NUM_CLASSES, \n",
        "    activation='softmax')\n",
        "predictions = dense(merged)\n",
        "\n",
        "model = Model(\n",
        "    inputs=[top_input, bm_input], \n",
        "    outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 300)      120000000   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          219648      embedding[0][0]                  \n",
            "                                                                 embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n",
            "                                                                 lstm[1][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            514         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 120,220,162\n",
            "Trainable params: 120,220,162\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVDolP2wEUj7"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "lr = 1e-3\n",
        "opt = Adam(lr=lr, decay=lr/50)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS8GUZAaF2Mp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "10b7ca54-a9dd-4596-e5ec-a9430c463ab3"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 50\n",
        "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
        "history = model.fit(x=[x1_train, x2_train],\n",
        "                    y=y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(\n",
        "                      [x1_val, x2_val], \n",
        "                      y_val\n",
        "                    ),\n",
        "                    shuffle=True,\n",
        "                    callbacks=stop,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.6607 - accuracy: 0.6089 - val_loss: 0.5571 - val_accuracy: 0.7751\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 17s 1s/step - loss: 0.3957 - accuracy: 0.8317 - val_loss: 0.3334 - val_accuracy: 0.8543\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 17s 1s/step - loss: 0.1934 - accuracy: 0.9292 - val_loss: 0.3698 - val_accuracy: 0.8508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igzeov9JGK-D"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model.predict(\n",
        "    [X1_test, X2_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkrplQnfG_ar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "aff76051-d836-492b-ee96-b60246fb98f9"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
        "print(len(y_pred))\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2494\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBcPWYOrGTur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "f83713f0-9501-4215-9445-4c9458382fc7"
      },
      "source": [
        "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
        "test_labels = test_df['stance'].tolist()\n",
        "n_test_labels = [(ele-1) for ele in test_labels]\n",
        "#print(y_pred)\n",
        "print('Accuracy is')\n",
        "print(metrics.accuracy_score(n_test_labels, y_pred)*100)\n",
        "print(classification_report(n_test_labels, y_pred, target_names = ['for', 'against']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is\n",
            "83.9214113873296\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         for       0.82      0.77      0.79      1007\n",
            "     against       0.85      0.89      0.87      1487\n",
            "\n",
            "    accuracy                           0.84      2494\n",
            "   macro avg       0.84      0.83      0.83      2494\n",
            "weighted avg       0.84      0.84      0.84      2494\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}