{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeH71tyrBSs4",
    "outputId": "c7b4b3e0-2fbf-42c3-8e18-1461089cc39b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH5U5sjaBVAV"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation, Add\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "id": "vNfqa33JBZ6l",
    "outputId": "f491f0df-c7ec-4c81-f227-e98cedd922be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
      "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
      "       'best_novelty', 'best_emotion'],\n",
      "      dtype='object')\n",
      "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
      "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
      "       'best_novelty', 'best_emotion'],\n",
      "      dtype='object')\n",
      "Index(['id', 'date', 'speaker', 'statement', 'sources',\n",
      "       'paragraph_based_content', 'fullText_based_content', 'label_fnn',\n",
      "       'best_novelty', 'best_emotion'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>sources</th>\n",
       "      <th>paragraph_based_content</th>\n",
       "      <th>fullText_based_content</th>\n",
       "      <th>label_fnn</th>\n",
       "      <th>best_novelty</th>\n",
       "      <th>best_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1678</td>\n",
       "      <td>2010-04-11T16:37:40-04:00</td>\n",
       "      <td>Jon Kyl</td>\n",
       "      <td>\"President Obama himself attempted to filibust...</td>\n",
       "      <td>['http://abcnews.go.com/ThisWeek/video/supreme...</td>\n",
       "      <td>['U.S. Supreme Court Justice John Paul Stevens...</td>\n",
       "      <td>U.S. Supreme Court Justice John Paul Stevens a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1820</td>\n",
       "      <td>2010-05-23T18:11:09-04:00</td>\n",
       "      <td>Michael Steele</td>\n",
       "      <td>In Hawaii, \"they don't have a history of throw...</td>\n",
       "      <td>['http://www.starbulletin.com/news/bulletin/94...</td>\n",
       "      <td>[\"On ABC's This Week, the chairmen of the Repu...</td>\n",
       "      <td>On ABC's This Week, the chairmen of the Republ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1624</td>\n",
       "      <td>2010-03-26T10:24:21-04:00</td>\n",
       "      <td>John Boehner</td>\n",
       "      <td>\"Our national debt ... is on track to exceed t...</td>\n",
       "      <td>['http://www.desmoinesregister.com/article/201...</td>\n",
       "      <td>['Ever since Barack Obama became president and...</td>\n",
       "      <td>Ever since Barack Obama became president and b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576</td>\n",
       "      <td>2010-03-12T11:45:14-05:00</td>\n",
       "      <td>America's Health Insurance Plans</td>\n",
       "      <td>\"Health insurance companies' costs are only 4 ...</td>\n",
       "      <td>['http://www.youtube.com/watch?v=4O8CxZ1OD58',...</td>\n",
       "      <td>[\"As the battle over health care reform approa...</td>\n",
       "      <td>As the battle over health care reform approach...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1770</td>\n",
       "      <td>2010-05-07T11:54:44-04:00</td>\n",
       "      <td>Michael Bloomberg</td>\n",
       "      <td>\"We can prevent terror suspects from boarding ...</td>\n",
       "      <td>['http://www.huffingtonpost.com/michael-bloomb...</td>\n",
       "      <td>['In the wake of a foiled car bomb attempt in ...</td>\n",
       "      <td>In the wake of a foiled car bomb attempt in Ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                       date  ... best_novelty best_emotion\n",
       "0  1678  2010-04-11T16:37:40-04:00  ...            0            0\n",
       "1  1820  2010-05-23T18:11:09-04:00  ...            1            1\n",
       "2  1624  2010-03-26T10:24:21-04:00  ...            1            0\n",
       "3  1576  2010-03-12T11:45:14-05:00  ...            1            0\n",
       "4  1770  2010-05-07T11:54:44-04:00  ...            1            1\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('FNID_Data/fnn_train.csv')\n",
    "print(train_df.columns)\n",
    "train_df.head()\n",
    "le = LabelEncoder()\n",
    "train_df['label_fnn'] = le.fit_transform(train_df['label_fnn'])\n",
    "train_df.head()\n",
    "\n",
    "# dev set\n",
    "dev_df = pd.read_csv('FNID_Data/fnn_dev.csv')\n",
    "print(dev_df.columns)\n",
    "dev_df['label_fnn'] = le.transform(dev_df['label_fnn'])\n",
    "dev_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('FNID_Data/fnn_test.csv')\n",
    "print(test_df.columns)\n",
    "test_df['label_fnn'] = le.transform(test_df['label_fnn'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-ebN_vwXDTW",
    "outputId": "d9977679-8d25-463f-ffaa-9c1f7fef280a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (15212, 768)\n",
      "Hypothesis (15212, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_fnd = np.load(\"FNID_Data/pre_bert_fnid.npy\")\n",
    "hyp_bert_fnd = np.load(\"FNID_Data/hyp_bert_fnid.npy\")\n",
    "print('Premise', pre_bert_fnd.shape)\n",
    "print('Hypothesis', hyp_bert_fnd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltQtOKYJ5nxS",
    "outputId": "18e30b6a-0d18-4ef3-89f9-524214f36055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (1058, 768)\n",
      "Hypothesis (1058, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_fnd_val = np.load(\"FNID_Data/pre_bert_dev_fnid.npy\")\n",
    "hyp_bert_fnd_val = np.load(\"FNID_Data/hyp_bert_dev_fnid.npy\")\n",
    "print('Premise', pre_bert_fnd_val.shape)\n",
    "print('Hypothesis', hyp_bert_fnd_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qlMPGsrXLth",
    "outputId": "0d9b53de-300b-4336-93fe-22c158c5480e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (1054, 768)\n",
      "Hypothesis (1054, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_fnd_test = np.load(\"FNID_Data/pre_bert_test_fnid.npy\")\n",
    "hyp_bert_fnd_test = np.load(\"FNID_Data/hyp_bert_test_fnid.npy\")\n",
    "print('Premise', pre_bert_fnd_test.shape)\n",
    "print('Hypothesis', hyp_bert_fnd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOBTbc8gB0um",
    "outputId": "3e6b48d0-3e45-4add-deee-671e0926b177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15204\n",
      "Train Length is 30415\n",
      "Dataset length is 34638\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['statement'].tolist()\n",
    "train_lst_2 = train_df['paragraph_based_content']\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "test_lst_1 = test_df['statement'].tolist()\n",
    "test_lst_2 = test_df['paragraph_based_content']\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "dev_lst_1 = dev_df['statement'].tolist()\n",
    "dev_lst_2 = dev_df['paragraph_based_content']\n",
    "uq_dv_1 = list(set(dev_lst_1))\n",
    "uq_dv_2 = list(set(dev_lst_2))\n",
    "dev_merged = uq_dv_1 + uq_dv_2\n",
    "total_dataset = train_merged + dev_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j57scYihBoKc"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJHuYleQB-F2"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = '../resources/glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer, embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HPijJ8iCBvV"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "\t# with open(input_file) as infile:\n",
    "\t# \tfor line in infile:\n",
    "\t# \t\tdata = line.split(',')\n",
    "\t# \t\ttext, annotation = data[2], data[1]\n",
    "\t\t\t\n",
    "\t# \t\tif annotation == \"MET\":\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"1\")\n",
    "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"0\")\n",
    "  X1 = input_df['paragraph_based_content'].tolist()\n",
    "  X2 = input_df['statement'].tolist()\n",
    "  Y = input_df['label_fnn'].tolist()\n",
    "  Y_nv = input_df[\"best_novelty\"].values\n",
    "  Y_em = input_df[\"best_emotion\"].values\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "\t# for i, s in enumerate(sequences):\n",
    "\t# \tsequences[i] = sequences[i][-250:]\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y_fnd = np.array(Y)\n",
    "  Y_nv = np.array(Y_nv)\n",
    "  Y_em = np.array(Y_em)\n",
    "\n",
    "  return X1, X2, Y_fnd, Y_nv, Y_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSELmRRfCfH-",
    "outputId": "3e2e58c0-63a1-4d19-d5d7-3dcee44698f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 34638 Sentences\n",
      "Considered  62687 Left  49697\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer, embeddings_index = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEyrwXjMCjKF",
    "outputId": "52a99cba-3d3d-4f96-8552-b3d93644055c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100\n",
    "# read ml data\n",
    "X1, X2, Y_fnd, Y_nv, Y_em = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBJdMKRh1IOA",
    "outputId": "9d5d0b07-c989-41e2-9cfe-5badec4a03de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_val, X2_val, Y_fnd_val, Y_nv_val, Y_em_val = get_data(tokenizer, MAX_LENGTH, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Qtttbb0Cn-l",
    "outputId": "642ba2d1-3d2c-4002-96a7-8c90ca92e74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_fnd_test, Y_nv_test, Y_em_test = get_data(tokenizer, MAX_LENGTH, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZpXXbsSCv0X"
   },
   "outputs": [],
   "source": [
    "# Scaffold labels\n",
    "novel = embeddings_index['original']\n",
    "duplicate = embeddings_index['duplicate']\n",
    "emotion_true = embeddings_index['anticipation']+embeddings_index['sadness']+embeddings_index['joy']+embeddings_index['trust']\n",
    "emotion_false = embeddings_index['anger']+embeddings_index['fear']+embeddings_index['disgust']+embeddings_index['surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXwsUBICC0Pm",
    "outputId": "c20c5344-058d-44a7-953c-419ef90e45aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (15212, 300)\n",
      "Test bias (1054, 300)\n",
      "Val bias (1058, 300)\n"
     ]
    }
   ],
   "source": [
    "# Novelty Bias\n",
    "true_train_labels = train_df['label_fnn'].tolist()\n",
    "true_test_labels = test_df['label_fnn'].tolist()\n",
    "train_bias_nv = []\n",
    "test_bias_nv = []\n",
    "val_bias_nv = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i, row in train_df.iterrows():\n",
    "    if row['best_novelty'] == 0 and row['label_fnn'] == 0:\n",
    "        train_bias_nv.append(novel)\n",
    "    elif row['best_novelty'] == 1 and row['label_fnn'] == 1:\n",
    "        train_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        train_bias_nv.append(zero_vector)\n",
    "for i, row in test_df.iterrows():\n",
    "    if row['best_novelty'] == 0 and row['label_fnn'] == 0:\n",
    "        test_bias_nv.append(novel)\n",
    "    elif row['best_novelty'] == 1 and row['label_fnn'] == 1:\n",
    "        test_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        test_bias_nv.append(zero_vector)\n",
    "for i, row in dev_df.iterrows():\n",
    "    if row['best_novelty'] == 0 and row['label_fnn'] == 0:\n",
    "        val_bias_nv.append(novel)\n",
    "    elif row['best_novelty'] == 1 and row['label_fnn'] == 1:\n",
    "        val_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        val_bias_nv.append(zero_vector)\n",
    "        #print('Error in Test please check')\n",
    "train_bias_nv = np.stack(train_bias_nv)\n",
    "test_bias_nv = np.stack(test_bias_nv)\n",
    "val_bias_nv = np.stack(val_bias_nv)\n",
    "print('Train bias', train_bias_nv.shape)\n",
    "print('Test bias', test_bias_nv.shape)\n",
    "print('Val bias', val_bias_nv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVfyaBViD0fR",
    "outputId": "87104dd0-dbb8-4dba-855b-e8e08f4e5574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (15212, 300)\n",
      "Test bias (1054, 300)\n",
      "Val bias (1058, 300)\n"
     ]
    }
   ],
   "source": [
    "# Emotion Bias\n",
    "train_bias_em = []\n",
    "test_bias_em = []\n",
    "val_bias_em = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i in range(len(train_df)):\n",
    "    pre = train_df.loc[i, 'best_emotion']\n",
    "    if train_df.loc[i, 'label_fnn'] == 0 and pre == 1:\n",
    "        train_bias_em.append(emotion_false)\n",
    "    elif train_df.loc[i, 'label_fnn'] == 1 and pre == 0:\n",
    "        train_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        train_bias_em.append(zero_vector)\n",
    "for i in range(len(test_df)):\n",
    "    pre = test_df.loc[i, 'best_emotion']\n",
    "    if test_df.loc[i, 'label_fnn'] == 0 and pre == 1:\n",
    "        test_bias_em.append(emotion_false)\n",
    "    elif test_df.loc[i, 'label_fnn'] == 1 and pre == 0:\n",
    "        test_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        test_bias_em.append(zero_vector)\n",
    "for i in range(len(dev_df)):\n",
    "    pre = dev_df.loc[i, 'best_emotion']\n",
    "    if dev_df.loc[i, 'label_fnn'] == 0 and pre == 1:\n",
    "        val_bias_em.append(emotion_false)\n",
    "    elif dev_df.loc[i, 'label_fnn'] == 1 and pre == 0:\n",
    "        val_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        val_bias_em.append(zero_vector)\n",
    "train_bias_em = np.stack(train_bias_em)\n",
    "test_bias_em = np.stack(test_bias_em)\n",
    "val_bias_em = np.stack(val_bias_em)\n",
    "print('Train bias', train_bias_em.shape)\n",
    "print('Test bias', test_bias_em.shape)\n",
    "print('Val bias', val_bias_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjUAGKJpEyKx"
   },
   "outputs": [],
   "source": [
    "# Considering the final train and test bias\n",
    "train_bias = np.add(train_bias_nv, train_bias_em)\n",
    "test_bias = np.add(test_bias_nv, test_bias_em)\n",
    "val_bias = np.add(val_bias_nv, val_bias_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcaDeb9tE083",
    "outputId": "c4ff5d79-8287-4b03-f345-18515f0fb398"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encodings\n",
    "y_train_nv = keras.utils.to_categorical(Y_nv)\n",
    "print(y_train_nv)\n",
    "y_train_em = keras.utils.to_categorical(Y_em)\n",
    "print(y_train_em)\n",
    "y_train_fnd = keras.utils.to_categorical(Y_fnd)\n",
    "print(y_train_fnd)\n",
    "y_test_nv = keras.utils.to_categorical(Y_nv_test)\n",
    "print(y_test_nv)\n",
    "y_test_em = keras.utils.to_categorical(Y_em_test)\n",
    "print(y_test_em)\n",
    "y_test_fnd = keras.utils.to_categorical(Y_fnd_test)\n",
    "print(y_test_fnd)\n",
    "y_val_nv = keras.utils.to_categorical(Y_nv_val)\n",
    "print(y_val_nv)\n",
    "y_val_em = keras.utils.to_categorical(Y_em_val)\n",
    "print(y_val_em)\n",
    "y_val_fnd = keras.utils.to_categorical(Y_fnd_val)\n",
    "print(y_val_fnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdAaFSzaFG3Z"
   },
   "outputs": [],
   "source": [
    "# Not required here\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# VALIDATION_RATIO = 0.1\n",
    "# RANDOM_STATE = 9527\n",
    "# x1_train, x1_val, \\\n",
    "# x2_train, x2_val, \\\n",
    "# x1_train_bert, x1_val_bert, \\\n",
    "# x2_train_bert, x2_val_bert, \\\n",
    "# y_train_nv, y_val_nv, \\\n",
    "# y_train_em, y_val_em, \\\n",
    "# y_train_fnd, y_val_fnd, \\\n",
    "# train_bias, val_bias, \\\n",
    "# train_bias_nv, val_bias_nv, \\\n",
    "# train_bias_em, val_bias_em = \\\n",
    "#     train_test_split(\n",
    "#         X1, X2, \n",
    "#         pre_bert_fnd, hyp_bert_fnd,\n",
    "#         y_train_nv, y_train_em,\n",
    "#         y_train_fnd, train_bias,\n",
    "#         train_bias_nv, train_bias_em, \n",
    "#         test_size=VALIDATION_RATIO, \n",
    "#         random_state=RANDOM_STATE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBxug_2MFMVI",
    "outputId": "bc473402-0072-461f-faa2-1aadaa6c3943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (15212, 100)\n",
      "x2_train: (15212, 100)\n",
      "y_train_cs : (15212, 2)\n",
      "Train_Bias : (15212, 300)\n",
      "----------\n",
      "x1_val:   (1058, 100)\n",
      "x2_val:   (1058, 100)\n",
      "y_val_cs :   (1058, 2)\n",
      "Val_Bias : (1058, 300)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {X1.shape}\")\n",
    "print(f\"x2_train: {X2.shape}\")\n",
    "print(f\"y_train_cs : {y_train_fnd.shape}\")\n",
    "print(f\"Train_Bias : {train_bias.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {X1_val.shape}\")\n",
    "print(f\"x2_val:   {X2_val.shape}\")\n",
    "print(f\"y_val_cs :   {y_val_fnd.shape}\")\n",
    "print(f\"Val_Bias : {val_bias.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naPNMlWxFRjW"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVheuA_4hcLB"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iF-xT_ThX14R",
    "outputId": "9b9b4c6a-7c9c-4647-9732-426285565f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 100, 300)     541200      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 600)     0           bi_lstm_0[0][0]                  \n",
      "                                                                 bi_lstm_0[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 768)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 768)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       [(None, 100, 300), ( 901200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1, 300)       1102800     reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           bi_lstm_1[0][1]                  \n",
      "                                                                 bi_lstm_1[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 600)       0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           ((None, 300), (None, 6031        bi_lstm_1[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          901200      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           tf.__operators__.add[0][0]       \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pre_nv (Dense)                  (None, 64)           38464       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_em (Dense)                  (None, 64)           38464       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_fnd (Dense)                 (None, 64)           38464       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nv (Dense)                      (None, 2)            130         pre_nv[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "em (Dense)                      (None, 2)            130         pre_em[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fnd (Dense)                     (None, 2)            130         pre_fnd[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 123,568,213\n",
      "Trainable params: 123,568,213\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT + Normal Grand Model\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "top_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM, weights = [embedding_matrix], trainable = True)\n",
    "top_embedded_wd = embedding_layer(\n",
    "    top_input_wd)\n",
    "bm_embedded_wd = embedding_layer(\n",
    "    bm_input_wd)\n",
    "\n",
    "source_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3), name=\"bi_lstm_0\")\n",
    "shared_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, return_state=True, activation='tanh', recurrent_dropout = 0.3), name=\"bi_lstm_1\")\n",
    "top_source_wd = source_lstm_wd(top_embedded_wd)\n",
    "bm_source_wd = source_lstm_wd(bm_embedded_wd)\n",
    "\n",
    "source_comb_wd = concatenate(\n",
    "    [top_source_wd, bm_source_wd],\n",
    "    axis=-1\n",
    "    )\n",
    "(lstm_ops_wd, forward_h, forward_c, backward_h, backward_c) = shared_lstm_wd(source_comb_wd)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector, attention_weights = Attention(10)(lstm_ops_wd, state_h)\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bias_input = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_nv = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_em = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #300D vector\n",
    "\n",
    "#merged = Add()([top_output, bm_output])\n",
    "#merged_bd = Add()([lstm_ops, bias_input])\n",
    "\n",
    "# Bert and Normal Combination\n",
    "comb_features = concatenate(\n",
    "    [context_vector+lstm_ops_bt, context_vector*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "comb_features_nv = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input_nv, context_vector*lstm_ops_bt*bias_input_nv],\n",
    "    axis=-1\n",
    "    )\n",
    "comb_features_em = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input_em, context_vector*lstm_ops_bt*bias_input_em],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "comb_features_fnd = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input, context_vector*lstm_ops_bt*bias_input],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "pre_nv = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_nv')(comb_features)\n",
    "\n",
    "pre_em = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_em')(comb_features)\n",
    "\n",
    "pre_fnd = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_fnd')(comb_features)\n",
    "\n",
    "dense_nv =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'nv')\n",
    "\n",
    "dense_em =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'em')\n",
    "\n",
    "dense_fnd =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'fnd')\n",
    "\n",
    "predictions_nv = dense_nv(pre_nv)\n",
    "predictions_em = dense_em(pre_em)\n",
    "predictions_fnd = dense_fnd(pre_fnd)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input_wd, bm_input_wd, top_input_bt, bm_input_bt, bias_input, bias_input_nv, bias_input_em], \n",
    "    outputs=[predictions_nv, predictions_em, predictions_fnd])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvR4vF2XGUAA"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'nv':'categorical_crossentropy', 'em':'categorical_crossentropy', 'fnd':'categorical_crossentropy'},\n",
    "    loss_weights={'nv': 0, 'em':0, 'fnd': 1},\n",
    "    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='multitask_fnidbias.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IehPCX70YbdE",
    "outputId": "c5fded40-8c88-4224-aec3-ac4171a1b9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 192s 3s/step - loss: 0.6923 - nv_loss: 0.6954 - em_loss: 0.6914 - fnd_loss: 0.6923 - nv_accuracy: 0.4920 - em_accuracy: 0.5347 - fnd_accuracy: 0.5797 - val_loss: 0.5930 - val_nv_loss: 0.7002 - val_em_loss: 0.6919 - val_fnd_loss: 0.5930 - val_nv_accuracy: 0.4480 - val_em_accuracy: 0.5302 - val_fnd_accuracy: 0.6711\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 173s 3s/step - loss: 0.5674 - nv_loss: 0.7036 - em_loss: 0.6919 - fnd_loss: 0.5674 - nv_accuracy: 0.4391 - em_accuracy: 0.5296 - fnd_accuracy: 0.6984 - val_loss: 0.5575 - val_nv_loss: 0.7017 - val_em_loss: 0.6921 - val_fnd_loss: 0.5575 - val_nv_accuracy: 0.4480 - val_em_accuracy: 0.5265 - val_fnd_accuracy: 0.7098\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 167s 3s/step - loss: 0.4681 - nv_loss: 0.6998 - em_loss: 0.6975 - fnd_loss: 0.4681 - nv_accuracy: 0.4596 - em_accuracy: 0.5081 - fnd_accuracy: 0.7720 - val_loss: 0.5829 - val_nv_loss: 0.6976 - val_em_loss: 0.6942 - val_fnd_loss: 0.5829 - val_nv_accuracy: 0.4773 - val_em_accuracy: 0.5217 - val_fnd_accuracy: 0.7013\n"
     ]
    }
   ],
   "source": [
    "# MultiTask BERT Model\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[X1, X2, pre_bert_fnd, hyp_bert_fnd, train_bias, train_bias_nv, train_bias_em],\n",
    "                    y=[y_train_nv, y_train_em, y_train_fnd],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [X1_val, X2_val, pre_bert_fnd_val, hyp_bert_fnd_val, val_bias, val_bias_nv, val_bias_em], \n",
    "                      [y_val_nv, y_val_em, y_val_fnd]\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4XksIFtGugN"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [X1_test, X2_test, pre_bert_fnd_test, hyp_bert_fnd_test, test_bias, test_bias_nv, test_bias_em])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_-4w8EnGvb1",
    "outputId": "2ebb648f-f3c9-4556-bb02-4e5623d89e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1054, 2)\n",
      "(1054, 2)\n",
      "(1054, 2)\n",
      "(1054, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.stack(predictions).shape)\n",
    "print(predictions[0].shape)\n",
    "print(predictions[1].shape)\n",
    "print(predictions[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwbuKT7emwRR"
   },
   "outputs": [],
   "source": [
    "# Result Labels\n",
    "res_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEA73dfiGxVv",
    "outputId": "28693bb6-cfab-4f4a-e6f3-6f25578c77a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNID Accuracy is\n",
      "85.7685009487666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.75      0.97      0.84       418\n",
      "        real       0.97      0.78      0.87       636\n",
      "\n",
      "    accuracy                           0.86      1054\n",
      "   macro avg       0.86      0.88      0.86      1054\n",
      "weighted avg       0.88      0.86      0.86      1054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[2], axis=1)]\n",
    "res_df['Fake_News_Labels'] = y_pred\n",
    "#print(y_pred)\n",
    "print('FNID Accuracy is')\n",
    "print(metrics.accuracy_score(Y_fnd_test, y_pred)*100)\n",
    "print(classification_report(Y_fnd_test, y_pred, target_names = ['fake', 'real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8ksnr6UGzJS",
    "outputId": "2e8fe4de-b274-4b35-afae-365b61e49060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Accuracy is\n",
      "44.11764705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.70      0.09      0.16       621\n",
      "       false       0.42      0.94      0.58       433\n",
      "\n",
      "    accuracy                           0.44      1054\n",
      "   macro avg       0.56      0.52      0.37      1054\n",
      "weighted avg       0.58      0.44      0.33      1054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[1], axis=1)]\n",
    "res_df['Emotion_Labels'] = y_pred\n",
    "print('Emotion Accuracy is')\n",
    "print(metrics.accuracy_score(Y_em_test, y_pred)*100)\n",
    "print(classification_report(Y_em_test, y_pred, target_names = ['true', 'false']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQIgPZVbG2LI",
    "outputId": "b205105e-19d0-48b3-bde7-9665d04ce7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NV Accuracy is\n",
      "36.62239089184061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       novel       0.50      0.23      0.31       667\n",
      "   duplicate       0.31      0.60      0.41       387\n",
      "\n",
      "    accuracy                           0.37      1054\n",
      "   macro avg       0.41      0.42      0.36      1054\n",
      "weighted avg       0.43      0.37      0.35      1054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[0], axis=1)]\n",
    "res_df['Novelty_Labels'] = y_pred\n",
    "print('NV Accuracy is')\n",
    "print(metrics.accuracy_score(Y_nv_test, y_pred)*100)\n",
    "print(classification_report(Y_nv_test, y_pred, target_names = ['novel', 'duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLtyGIDrnBuP"
   },
   "outputs": [],
   "source": [
    "# Saving the labels\n",
    "res_df.to_csv(\"Attn_MtaskRes_Fnid.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Attention_FNID_Multitask.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
