{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeH71tyrBSs4",
    "outputId": "1c6200b6-5775-45d9-9222-a8e11f0ba144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH5U5sjaBVAV"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation, Add\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "vNfqa33JBZ6l",
    "outputId": "ddea4cd9-1450-4f80-cc6a-e1da8cac6c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'Body', 'Headline', 'Stance', 'Emotion_Labels',\n",
      "       'Novelty_Quora', 'sentiment_scores_pre', 'sentiment_scores_hyp',\n",
      "       'sentiment_pre_labels', 'sentiment_hyp_labels', 'Combined_Sentiment',\n",
      "       'com_femotion', 'best_sentiments'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'id', 'Body', 'Headline', 'Stance', 'Emotion_Labels',\n",
      "       'Novelty_Quora', 'sentiment_scores_pre', 'sentiment_scores_hyp',\n",
      "       'sentiment_pre_labels', 'sentiment_hyp_labels', 'Combined_Sentiment',\n",
      "       'com_femotion', 'best_sentiments'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Body</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Emotion_Labels</th>\n",
       "      <th>Novelty_Quora</th>\n",
       "      <th>sentiment_scores_pre</th>\n",
       "      <th>sentiment_scores_hyp</th>\n",
       "      <th>sentiment_pre_labels</th>\n",
       "      <th>sentiment_hyp_labels</th>\n",
       "      <th>Combined_Sentiment</th>\n",
       "      <th>com_femotion</th>\n",
       "      <th>best_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1964</td>\n",
       "      <td>Last week, Apple sent out the invites for its ...</td>\n",
       "      <td>EXCLUSIVE: Apple To Unveil The Long-Awaited Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088941</td>\n",
       "      <td>0.316288</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1618</td>\n",
       "      <td>Did a woman claiming to have a third breast pl...</td>\n",
       "      <td>3-Boobed Woman a Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.143214</td>\n",
       "      <td>-0.140901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2316</td>\n",
       "      <td>A fourth grader named Aiden Steward was suspen...</td>\n",
       "      <td>Texas Boy Suspended For 'Threatening' Classmat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.154774</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1452</td>\n",
       "      <td>LOS ANGELES (CBS Seattle/AP) — A scorpion stun...</td>\n",
       "      <td>Woman stung by scorpion on Alaska Airlines flight</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116980</td>\n",
       "      <td>-0.008654</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1636</td>\n",
       "      <td>If the bizarre story about Joan Rivers' doctor...</td>\n",
       "      <td>Disgusting! Joan Rivers Doc Gwen Korovin’s Sic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.054843</td>\n",
       "      <td>-0.557444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id  ... com_femotion best_sentiments\n",
       "0           0  1964  ...            1               0\n",
       "1           1  1618  ...            0               0\n",
       "2           2  2316  ...            1               0\n",
       "3           3  1452  ...            0               1\n",
       "4           4  1636  ...            0               1\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('FNC_Data/train_fnc_agdgonly.csv')\n",
    "print(train_df.columns)\n",
    "train_df.head()\n",
    "le = LabelEncoder()\n",
    "train_df['Stance'] = le.fit_transform(train_df['Stance'])\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('FNC_Data/test_fnc_agdgonly.csv')\n",
    "print(test_df.columns)\n",
    "test_df['Stance'] = le.transform(test_df['Stance'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-ebN_vwXDTW",
    "outputId": "f226b26d-2034-4f6b-cb23-60404f5cd6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (4518, 768)\n",
      "Hypothesis (4518, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_fnc = np.load(\"FNC_Data/pre_bert_fnc.npy\")\n",
    "hyp_bert_fnc = np.load(\"FNC_Data/hyp_bert_fnc.npy\")\n",
    "print('Premise', pre_bert_fnc.shape)\n",
    "print('Hypothesis', hyp_bert_fnc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qlMPGsrXLth",
    "outputId": "797831eb-103f-42e3-bcf8-629f85f2adb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (2600, 768)\n",
      "Hypothesis (2600, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_fnc_test = np.load(\"FNC_Data/pre_bert_test_fnc.npy\")\n",
    "hyp_bert_fnc_test = np.load(\"FNC_Data/hyp_bert_test_fnc.npy\")\n",
    "print('Premise', pre_bert_fnc_test.shape)\n",
    "print('Hypothesis', hyp_bert_fnc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOBTbc8gB0um",
    "outputId": "280e8ffb-ae9f-4446-de9f-df3a689e14d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518\n",
      "4518\n",
      "842\n",
      "958\n",
      "Train Length is 1800\n",
      "Test merged 961\n",
      "Dataset length is 2761\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['Body'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "train_lst_2 = train_df['Headline'].tolist()\n",
    "print(len(train_lst_2))\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "print(len(uq_tr_2))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['Body'].tolist()\n",
    "test_lst_2 = test_df['Headline'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j57scYihBoKc"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJHuYleQB-F2"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = '../resouces/glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer, embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HPijJ8iCBvV"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "\t# with open(input_file) as infile:\n",
    "\t# \tfor line in infile:\n",
    "\t# \t\tdata = line.split(',')\n",
    "\t# \t\ttext, annotation = data[2], data[1]\n",
    "\t\t\t\n",
    "\t# \t\tif annotation == \"MET\":\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"1\")\n",
    "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"0\")\n",
    "  X1 = input_df['Body'].tolist()\n",
    "  X2 = input_df['Headline'].tolist()\n",
    "  Y = input_df['Stance'].tolist()\n",
    "  Y_nv = input_df[\"Novelty_Quora\"].values\n",
    "  Y_em = input_df[\"com_femotion\"].values\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "\t# for i, s in enumerate(sequences):\n",
    "\t# \tsequences[i] = sequences[i][-250:]\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y_fnc = np.array(Y)\n",
    "  Y_nv = np.array(Y_nv)\n",
    "  Y_em = np.array(Y_em)\n",
    "\n",
    "  return X1, X2, Y_fnc, Y_nv, Y_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSELmRRfCfH-",
    "outputId": "d622a27a-12af-4119-a9ca-76e2dfde92a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 2761 Sentences\n",
      "Considered  20628 Left  7234\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer, embeddings_index = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEyrwXjMCjKF",
    "outputId": "b888f707-cccb-4abe-f2c3-e5cbf5078ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100\n",
    "# read ml data\n",
    "X1, X2, Y_fnc, Y_nv, Y_em = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Qtttbb0Cn-l",
    "outputId": "cf0fbb78-1d61-4435-e636-d607d981248a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_fnc_test, Y_nv_test, Y_em_test = get_data(tokenizer, MAX_LENGTH, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZpXXbsSCv0X"
   },
   "outputs": [],
   "source": [
    "# Scaffold labels\n",
    "novel = embeddings_index['original']\n",
    "duplicate = embeddings_index['duplicate']\n",
    "emotion_true = embeddings_index['anticipation']+embeddings_index['sadness']+embeddings_index['joy']+embeddings_index['trust']\n",
    "emotion_false = embeddings_index['anger']+embeddings_index['fear']+embeddings_index['disgust']+embeddings_index['surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXwsUBICC0Pm",
    "outputId": "dbe8b51a-597e-4f78-f397-b1af58b7252c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (4518, 300)\n",
      "Test bias (2600, 300)\n"
     ]
    }
   ],
   "source": [
    "# Novelty Bias\n",
    "true_train_labels = train_df['Stance'].tolist()\n",
    "true_test_labels = test_df['Stance'].tolist()\n",
    "train_bias_nv = []\n",
    "test_bias_nv = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i, row in train_df.iterrows():\n",
    "    if row['Novelty_Quora'] == 0 and row['Stance'] == 1:\n",
    "        train_bias_nv.append(novel)\n",
    "    elif row['Novelty_Quora'] == 1 and row['Stance'] == 0:\n",
    "        train_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        train_bias_nv.append(zero_vector)\n",
    "for i, row in test_df.iterrows():\n",
    "    if row['Novelty_Quora'] == 0 and row['Stance'] == 1:\n",
    "        test_bias_nv.append(novel)\n",
    "    elif row['Novelty_Quora'] == 1 and row['Stance'] == 0:\n",
    "        test_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        test_bias_nv.append(zero_vector)\n",
    "        #print('Error in Test please check')\n",
    "train_bias_nv = np.stack(train_bias_nv)\n",
    "test_bias_nv = np.stack(test_bias_nv)\n",
    "print('Train bias', train_bias_nv.shape)\n",
    "print('Test bias', test_bias_nv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVfyaBViD0fR",
    "outputId": "61f5b91f-b546-4986-ecb3-af3a3c3c8a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (4518, 300)\n",
      "Test bias (2600, 300)\n"
     ]
    }
   ],
   "source": [
    "# Emotion Bias\n",
    "train_bias_em = []\n",
    "test_bias_em = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i in range(len(train_df)):\n",
    "    pre = train_df.loc[i, 'com_femotion']\n",
    "    if train_df.loc[i, 'Stance'] == 0 and pre == 1:\n",
    "        train_bias_em.append(emotion_true)\n",
    "    elif train_df.loc[i, 'Stance'] == 1 and pre == 0:\n",
    "        train_bias_em.append(emotion_false)\n",
    "    else:\n",
    "        train_bias_em.append(zero_vector)\n",
    "for i in range(len(test_df)):\n",
    "    pre = test_df.loc[i, 'com_femotion']\n",
    "    if test_df.loc[i, 'Stance'] == 0 and pre == 1:\n",
    "        test_bias_em.append(emotion_true)\n",
    "    elif test_df.loc[i, 'Stance'] == 1 and pre == 0:\n",
    "        test_bias_em.append(emotion_false)\n",
    "    else:\n",
    "        test_bias_em.append(zero_vector)\n",
    "train_bias_em = np.stack(train_bias_em)\n",
    "test_bias_em = np.stack(test_bias_em)\n",
    "print('Train bias', train_bias_em.shape)\n",
    "print('Test bias', test_bias_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjUAGKJpEyKx"
   },
   "outputs": [],
   "source": [
    "# Considering the final train and test bias\n",
    "train_bias = np.add(train_bias_nv, train_bias_em)\n",
    "test_bias = np.add(test_bias_nv, test_bias_em)\n",
    "# train_bias = np.add(train_bias_nv, train_bias_em, train_bias_st)\n",
    "# test_bias = np.add(test_bias_nv, test_bias_em, test_bias_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcaDeb9tE083",
    "outputId": "0987564e-30e1-4ce6-92dc-2b7a44d24a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encodings\n",
    "y_train_nv = keras.utils.to_categorical(Y_nv)\n",
    "print(y_train_nv)\n",
    "y_train_em = keras.utils.to_categorical(Y_em)\n",
    "print(y_train_em)\n",
    "y_train_fnc = keras.utils.to_categorical(Y_fnc)\n",
    "print(y_train_fnc)\n",
    "y_test_nv = keras.utils.to_categorical(Y_nv_test)\n",
    "print(y_test_nv)\n",
    "y_test_em = keras.utils.to_categorical(Y_em_test)\n",
    "print(y_test_em)\n",
    "y_test_fnc = keras.utils.to_categorical(Y_fnc_test)\n",
    "print(y_test_fnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdAaFSzaFG3Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "x1_train_bert, x1_val_bert, \\\n",
    "x2_train_bert, x2_val_bert, \\\n",
    "y_train_nv, y_val_nv, \\\n",
    "y_train_em, y_val_em, \\\n",
    "y_train_fnc, y_val_fnc, \\\n",
    "train_bias, val_bias, \\\n",
    "train_bias_nv, val_bias_nv, \\\n",
    "train_bias_em, val_bias_em = \\\n",
    "    train_test_split(\n",
    "        X1, X2, \n",
    "        pre_bert_fnc, hyp_bert_fnc,\n",
    "        y_train_nv, y_train_em,\n",
    "        y_train_fnc, train_bias,\n",
    "        train_bias_nv, train_bias_em, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBxug_2MFMVI",
    "outputId": "23d13835-081f-498c-e086-709af8c3ad1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (4066, 100)\n",
      "x2_train: (4066, 100)\n",
      "y_train_cs : (4066, 2)\n",
      "Train_Bias : (4066, 300)\n",
      "----------\n",
      "x1_val:   (452, 100)\n",
      "x2_val:   (452, 100)\n",
      "y_val_cs :   (452, 2)\n",
      "Val_Bias : (452, 300)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train_cs : {y_train_fnc.shape}\")\n",
    "print(f\"Train_Bias : {train_bias.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val_cs :   {y_val_fnc.shape}\")\n",
    "print(f\"Val_Bias : {val_bias.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naPNMlWxFRjW"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVheuA_4hcLB"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iF-xT_ThX14R",
    "outputId": "607fee41-45fa-4aeb-f8e9-c9b471089ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 100, 300)     541200      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 600)     0           bi_lstm_0[0][0]                  \n",
      "                                                                 bi_lstm_0[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 768)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 768)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       [(None, 100, 300), ( 901200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1, 300)       1102800     reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           bi_lstm_1[0][1]                  \n",
      "                                                                 bi_lstm_1[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 600)       0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           ((None, 300), (None, 6031        bi_lstm_1[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          901200      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 300)          0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 300)          0           tf.math.multiply[0][0]           \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 300)          0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 300)          0           tf.math.multiply_2[0][0]         \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 600)          0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 300)          0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pre_nv (Dense)                  (None, 64)           38464       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_em (Dense)                  (None, 64)           38464       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_fnc (Dense)                 (None, 64)           19264       tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nv (Dense)                      (None, 2)            130         pre_nv[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "em (Dense)                      (None, 2)            130         pre_em[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fnc (Dense)                     (None, 2)            130         pre_fnc[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 123,549,013\n",
      "Trainable params: 123,549,013\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT + Normal Grand Model\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "top_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM, weights = [embedding_matrix], trainable = True)\n",
    "top_embedded_wd = embedding_layer(\n",
    "    top_input_wd)\n",
    "bm_embedded_wd = embedding_layer(\n",
    "    bm_input_wd)\n",
    "\n",
    "source_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3), name=\"bi_lstm_0\")\n",
    "shared_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, return_state=True, activation='tanh', recurrent_dropout = 0.3), name=\"bi_lstm_1\")\n",
    "top_source_wd = source_lstm_wd(top_embedded_wd)\n",
    "bm_source_wd = source_lstm_wd(bm_embedded_wd)\n",
    "\n",
    "source_comb_wd = concatenate(\n",
    "    [top_source_wd, bm_source_wd],\n",
    "    axis=-1\n",
    "    )\n",
    "(lstm_ops_wd, forward_h, forward_c, backward_h, backward_c) = shared_lstm_wd(source_comb_wd)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector, attention_weights = Attention(10)(lstm_ops_wd, state_h)\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bias_input = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_nv = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_em = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #300D vector\n",
    "\n",
    "#merged = Add()([top_output, bm_output])\n",
    "#merged_bd = Add()([lstm_ops, bias_input])\n",
    "\n",
    "# Bert and Normal Combination\n",
    "comb_features = context_vector + lstm_ops_bt\n",
    "\n",
    "comb_features_nv = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input_nv, context_vector*lstm_ops_bt*bias_input_nv],\n",
    "    axis=-1\n",
    "    )\n",
    "comb_features_em = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input_em, context_vector*lstm_ops_bt*bias_input_em],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "comb_features_fnc = concatenate(\n",
    "    [context_vector+lstm_ops_bt+bias_input, context_vector*lstm_ops_bt*bias_input],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "pre_nv = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_nv')(comb_features_nv)\n",
    "\n",
    "pre_em = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_em')(comb_features_em)\n",
    "\n",
    "pre_fnc = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_fnc')(comb_features)\n",
    "\n",
    "dense_nv =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'nv')\n",
    "\n",
    "dense_em =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'em')\n",
    "\n",
    "dense_fnc =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'fnc')\n",
    "\n",
    "predictions_nv = dense_nv(pre_nv)\n",
    "predictions_em = dense_em(pre_em)\n",
    "predictions_fnc = dense_fnc(pre_fnc)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input_wd, bm_input_wd, top_input_bt, bm_input_bt, bias_input, bias_input_nv, bias_input_em], \n",
    "    outputs=[predictions_nv, predictions_em, predictions_fnc])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvR4vF2XGUAA"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'nv':'categorical_crossentropy', 'em':'categorical_crossentropy', 'fnc':'categorical_crossentropy'},\n",
    "    loss_weights={'nv': 0, 'em':0, 'fnc': 1},\n",
    "    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='multitask_fncbias.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IehPCX70YbdE",
    "outputId": "aad702e3-f3a4-40de-9870-357e553a7669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 70s 3s/step - loss: 0.5741 - nv_loss: 0.6598 - em_loss: 0.4662 - fnc_loss: 0.5741 - nv_accuracy: 0.7549 - em_accuracy: 0.6674 - fnc_accuracy: 0.7686 - val_loss: 0.4391 - val_nv_loss: 0.6542 - val_em_loss: 0.4245 - val_fnc_loss: 0.4391 - val_nv_accuracy: 0.8296 - val_em_accuracy: 0.7544 - val_fnc_accuracy: 0.8429\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.4639 - nv_loss: 0.6633 - em_loss: 0.4555 - fnc_loss: 0.4639 - nv_accuracy: 0.7817 - em_accuracy: 0.6969 - fnc_accuracy: 0.8114 - val_loss: 0.4009 - val_nv_loss: 0.6607 - val_em_loss: 0.4277 - val_fnc_loss: 0.4009 - val_nv_accuracy: 0.8186 - val_em_accuracy: 0.7013 - val_fnc_accuracy: 0.8429\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.3915 - nv_loss: 0.6842 - em_loss: 0.4590 - fnc_loss: 0.3915 - nv_accuracy: 0.6701 - em_accuracy: 0.6674 - fnc_accuracy: 0.8264 - val_loss: 0.3303 - val_nv_loss: 0.6995 - val_em_loss: 0.4313 - val_fnc_loss: 0.3303 - val_nv_accuracy: 0.4823 - val_em_accuracy: 0.6947 - val_fnc_accuracy: 0.8473\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.3164 - nv_loss: 0.7131 - em_loss: 0.4553 - fnc_loss: 0.3164 - nv_accuracy: 0.4816 - em_accuracy: 0.6662 - fnc_accuracy: 0.8616 - val_loss: 0.2885 - val_nv_loss: 0.7563 - val_em_loss: 0.4294 - val_fnc_loss: 0.2885 - val_nv_accuracy: 0.3496 - val_em_accuracy: 0.6991 - val_fnc_accuracy: 0.8739\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.2377 - nv_loss: 0.7939 - em_loss: 0.4501 - fnc_loss: 0.2377 - nv_accuracy: 0.3089 - em_accuracy: 0.6718 - fnc_accuracy: 0.8938 - val_loss: 0.2484 - val_nv_loss: 0.8632 - val_em_loss: 0.4294 - val_fnc_loss: 0.2484 - val_nv_accuracy: 0.2721 - val_em_accuracy: 0.6991 - val_fnc_accuracy: 0.8827\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.1851 - nv_loss: 0.8970 - em_loss: 0.4670 - fnc_loss: 0.1851 - nv_accuracy: 0.2461 - em_accuracy: 0.6497 - fnc_accuracy: 0.9141 - val_loss: 0.2297 - val_nv_loss: 0.9029 - val_em_loss: 0.4339 - val_fnc_loss: 0.2297 - val_nv_accuracy: 0.2765 - val_em_accuracy: 0.7124 - val_fnc_accuracy: 0.9071\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.1250 - nv_loss: 0.9284 - em_loss: 0.4662 - fnc_loss: 0.1250 - nv_accuracy: 0.2316 - em_accuracy: 0.6557 - fnc_accuracy: 0.9496 - val_loss: 0.1937 - val_nv_loss: 0.8289 - val_em_loss: 0.4341 - val_fnc_loss: 0.1937 - val_nv_accuracy: 0.2987 - val_em_accuracy: 0.6925 - val_fnc_accuracy: 0.9314\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 0.0821 - nv_loss: 0.8927 - em_loss: 0.4548 - fnc_loss: 0.0821 - nv_accuracy: 0.2661 - em_accuracy: 0.6622 - fnc_accuracy: 0.9673 - val_loss: 0.1592 - val_nv_loss: 0.9193 - val_em_loss: 0.4346 - val_fnc_loss: 0.1592 - val_nv_accuracy: 0.2566 - val_em_accuracy: 0.6858 - val_fnc_accuracy: 0.9580\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.0617 - nv_loss: 0.9373 - em_loss: 0.4716 - fnc_loss: 0.0617 - nv_accuracy: 0.2524 - em_accuracy: 0.6394 - fnc_accuracy: 0.9753 - val_loss: 0.1816 - val_nv_loss: 0.9404 - val_em_loss: 0.4368 - val_fnc_loss: 0.1816 - val_nv_accuracy: 0.2588 - val_em_accuracy: 0.6925 - val_fnc_accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "# MultiTask BERT Model\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, x1_train_bert, x2_train_bert, train_bias, train_bias_nv, train_bias_em],\n",
    "                    y=[y_train_nv, y_train_em, y_train_fnc],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, x1_val_bert, x2_val_bert, val_bias, val_bias_nv, val_bias_em], \n",
    "                      [y_val_nv, y_val_em, y_val_fnc]\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4XksIFtGugN"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [X1_test, X2_test, pre_bert_fnc_test, hyp_bert_fnc_test, test_bias, test_bias_nv, test_bias_em])\n",
    "# print('Accuracy is')\n",
    "# print(metrics.accuracy_score(y_test, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "# print(classification_report(y_test, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_-4w8EnGvb1",
    "outputId": "21d5457b-b648-4145-c33f-82e48465cc54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2600, 2)\n",
      "(2600, 2)\n",
      "(2600, 2)\n",
      "(2600, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.stack(predictions).shape)\n",
    "print(predictions[0].shape)\n",
    "print(predictions[1].shape)\n",
    "print(predictions[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwbuKT7emwRR"
   },
   "outputs": [],
   "source": [
    "# Result Labels\n",
    "res_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEA73dfiGxVv",
    "outputId": "a4dad9d7-5a44-4179-f110-803476189307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNC Accuracy is\n",
      "73.26923076923076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.75      0.96      0.84      1903\n",
      "    disagree       0.51      0.12      0.19       697\n",
      "\n",
      "    accuracy                           0.73      2600\n",
      "   macro avg       0.63      0.54      0.52      2600\n",
      "weighted avg       0.68      0.73      0.67      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[2], axis=1)]\n",
    "res_df['Fake_News_Labels'] = y_pred\n",
    "#print(y_pred)\n",
    "print('FNC Accuracy is')\n",
    "print(metrics.accuracy_score(Y_fnc_test, y_pred)*100)\n",
    "print(classification_report(Y_fnc_test, y_pred, target_names = ['agree', 'disagree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8ksnr6UGzJS",
    "outputId": "01b7b452-78e1-410c-ef43-30e6878ceae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Accuracy is\n",
      "61.42307692307693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.71      0.01      0.03      1012\n",
      "       false       0.61      1.00      0.76      1588\n",
      "\n",
      "    accuracy                           0.61      2600\n",
      "   macro avg       0.66      0.51      0.39      2600\n",
      "weighted avg       0.65      0.61      0.48      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[1], axis=1)]\n",
    "res_df['Emotion_Labels'] = y_pred\n",
    "print('Emotion Accuracy is')\n",
    "print(metrics.accuracy_score(Y_em_test, y_pred)*100)\n",
    "print(classification_report(Y_em_test, y_pred, target_names = ['true', 'false']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQIgPZVbG2LI",
    "outputId": "9306fdc5-d751-4fbf-f70e-cb8cef059033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NV Accuracy is\n",
      "43.88461538461538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       novel       0.42      0.87      0.57      1116\n",
      "   duplicate       0.54      0.11      0.19      1484\n",
      "\n",
      "    accuracy                           0.44      2600\n",
      "   macro avg       0.48      0.49      0.38      2600\n",
      "weighted avg       0.49      0.44      0.35      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[0], axis=1)]\n",
    "res_df['Novelty_Labels'] = y_pred\n",
    "print('NV Accuracy is')\n",
    "print(metrics.accuracy_score(Y_nv_test, y_pred)*100)\n",
    "print(classification_report(Y_nv_test, y_pred, target_names = ['novel', 'duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLtyGIDrnBuP"
   },
   "outputs": [],
   "source": [
    "# Saving the labels\n",
    "res_df.to_csv(\"MultiFNC_MtaskRes.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Attention_FNC_Multitask.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
